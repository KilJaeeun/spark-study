{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "08ce8e35",
   "metadata": {},
   "source": [
    "#  구조적 API: DataFrame, SQL, Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e4d08b7",
   "metadata": {},
   "source": [
    "구조적 API(Structured API)는 비정형 로그파일로부터 반정형 CSV파일, 정형적인 Parquet 파일까지 다양한 데이터를 처리할 수 있다.\n",
    "\n",
    "구조적 API에는 다음과 같은 3가지 분산 컬렉션 API가 있습니다.\n",
    "* Datasets\n",
    "* DataFrames\n",
    "* SQLtables and Views\n",
    "\n",
    "배치(Batch)와 스트리밍(Streaming) 처리에 구조적 API를 사용할 수 있습니다. 구조적 API를 활용하면 배치 작업을 스트리밍 작업으로 손쉽게 변환할 수 있습니다.\n",
    "\n",
    "구조적 API는 데이터 흐름을 정의하는 기본 추상화 개념입니다. \n",
    "* 타입형, 비타입형 API 의 개념과 차이점\n",
    "* 핵심 용어\n",
    "* 스파크가 구조적 API의 데이터 흐름을 해석하고 클러스터에서 실행하는 방식\n",
    "\n",
    "> 스파크는 트랜스포메이션의 처리과정을 정의하는 분산 프로그래밍 모델입니다. 사용자가 정의한 다수의 트랜스포메이셔은 지향성 비순환 그래프(DAG)로 표현되는 명령을 만들어냅니다. 액션은 하나의 잡을 클러스터에서 실행하기 위해 스테이지와 태스크로 나누고, DAG 처리 프로세스를 진행합니다. 트랜스포메이션과 액션으로 다루는 구조가 바로 DataFramerhk Dataset입니다. 새로운 DataFrame이나 Dataset을 만들려면 트랜스포메이션을 호출해야합니다. 그리고 연산을 시작하거나 사용한 언어에 맞는 데이터 타입으로  변환하려면 액션을 호출해야합니다.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "## DataFrames and Datasets\n",
    "(구분이 안돼서 https://timewizhan.tistory.com/entry/Spark-RDD-vs-Dataframes-vs-Datasets 참고함)\n",
    "Spark는 DataFrame과 Dataset이라는 두 가지 구조화된 컬렉션 개념을 가지고 있습니다. \n",
    "\n",
    "DataFrame과 Dataset은 잘 정의된 로우와 컬럼을 가지는 분산 테이블 형태의 컬렉션입니다. 각 컬럼은 다른 컬럼과 동일한 수의 로우를 가져야합니다. 컬렉션의 모든 로우는 같은 데이터 타입 정보를 가지고 있어야합니다. DataFrame과 Dataset은 결과를 생성하기 위해 어떤 데이터 연산을 적용해야하는지 정의하는 지연 연산의 실행계획이고, 불변성을 가집니다.\n",
    "> 기본적으로 테이블과 뷰는 DataFrame과 같습니다.\n",
    "\n",
    "DataFrame에 액션을 호출하면 Spark는 트랜스포메이션을 실제로 실행하고 결과를 반환합니다.\n",
    "DataFrame과 Dataset을 조금 더 구체적으로 정의하려면 스키마(Schema)를 알아야합니다. 스키마는 분산 컬렉션에 저장할때 데이터타입을 정의하는 방법입니다.\n",
    "\n",
    "\n",
    "## Schemas\n",
    "스키마는 DataFrame의 컬럼명과 데이터타입을 정의합니다. 스키마는 데이터소스에서 얻거나 직접 정의할 수 있습니다. 스키마는 여러 데이터 타입으로 구성되므로 어떤 데이터타입이 어느 위치에 있는지 정의하는 방법이 필요합니다.\n",
    "\n",
    "\n",
    "## Overview of Structured Spark Types\n",
    "Spark는 사실상 프로그래밍 언어입니다. Spark 는 실행 계획과 처리에 사용하는 자체 데이터타입 정보를 가지고 있는 카탈리스트(Catalyst)엔진을 사용합니다.\n",
    "\n",
    "카탈리스트 엔진은 다양한 실행퇴적화 기능을 제공합니다.\n",
    "\n",
    "Spark는 자체 데이터 타입을 지원하는 여러 언어 API와 직접 매핑되어 각 언어에 대한 매핑 테이블을 가지고 있습니다. Python 이나 R을 이용해 Spark의 구조적 API를 사용하더라도 대부분의 연산은 Python 이나 R의 데이터 타입이 아닌 Spark 의 데이터 타입을 사용합니다.\n",
    "아래의 예는 Spark 의 덧셈연산입니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f444f09a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[(number + 10): bigint]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df= spark.range(500).toDF(\"number\")\n",
    "df.select(df[\"number\"]+10)\n",
    "\n",
    "# spark 에서 덧셈 연산이 수행되는 이유는 Spark가 \n",
    "# 지원하는 언어를 이용해 작성된 표현식을 카탈리스트 엔진에서 Spark 의\n",
    "# 엔진에서 Spark의 데이터 타입으로 반환해 명령을 처리하기 때문이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0018c8eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e3e20e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67ec76dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe0d590c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eee63e6f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61be14a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59a232b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbea7d86",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32f3eb81",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66a31272",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68fcad01",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22bb58d2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
